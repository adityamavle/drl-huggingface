{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKC2wTuKpfZlB1HIFsFeu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityamavle/drl-huggingface/blob/master/learning_strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Monte Carlo method"
      ],
      "metadata": {
        "id": "HIzbN8tSb1Ce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLkpqnOubfZb",
        "outputId": "68857fd6-49b6-4066-829b-48364e7f3820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo State Values:\n",
            "[[1. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "n_states = 9\n",
        "n_actions = 4\n",
        "goal_state = 8\n",
        "\n",
        "# Initialize state values\n",
        "V = np.zeros(n_states)\n",
        "\n",
        "# Define the policy (for simplicity, using a random policy)\n",
        "policy = np.ones((n_states, n_actions)) / n_actions\n",
        "\n",
        "# Function to generate an episode\n",
        "def generate_episode():\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    state = 0  # Start from the initial state\n",
        "    while state != goal_state:\n",
        "        action = np.random.choice(range(n_actions), p=policy[state])\n",
        "        next_state = state + (action - 1) * (action % 2)  # Update based on action (left, right, up, down)\n",
        "        reward = 0 if next_state != goal_state else 1  # +1 reward upon reaching the goal\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    return states, actions, rewards\n",
        "\n",
        "# Monte Carlo algorithm\n",
        "num_episodes = 1000\n",
        "returns_sum = np.zeros(n_states)\n",
        "returns_count = np.zeros(n_states)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    states, actions, rewards = generate_episode()\n",
        "    G = 0\n",
        "    for t in range(len(states) - 1, -1, -1):\n",
        "        G = rewards[t] + G\n",
        "        state = states[t]\n",
        "        if state not in states[:t]:  # First visit Monte Carlo\n",
        "            returns_sum[state] += G\n",
        "            returns_count[state] += 1\n",
        "            V[state] = returns_sum[state] / returns_count[state]\n",
        "\n",
        "# Print the learned state values\n",
        "print(\"Monte Carlo State Values:\")\n",
        "print(V.reshape(3, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporal Difference"
      ],
      "metadata": {
        "id": "QJsEQSRIb50V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the environment\n",
        "n_states = 9\n",
        "n_actions = 4\n",
        "goal_state = 8\n",
        "\n",
        "# Initialize Q-values\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Epsilon-greedy exploration parameter\n",
        "\n",
        "# Function to choose an action based on epsilon-greedy policy\n",
        "def choose_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(range(n_actions))\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "# Q-learning algorithm\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = 0  # Start from the initial state\n",
        "    while state != goal_state:\n",
        "        action = choose_action(state)\n",
        "        next_state = state + (action - 1) * (action % 2)  # Update based on action (left, right, up, down)\n",
        "        reward = 0 if next_state != goal_state else 1  # +1 reward upon reaching the goal\n",
        "\n",
        "        # Q-value update\n",
        "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "# Print the learned Q-values\n",
        "print(\"Temporal Difference (Q-learning) Q-values:\")\n",
        "print(Q.reshape(n_states, n_actions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS1p2Alubmv2",
        "outputId": "2bc24275-c8fa-4e59-95b7-dae07c7411fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporal Difference (Q-learning) Q-values:\n",
            "[[0.62916823 0.58683668 0.56727745 0.729     ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.68078591 0.63935018 0.66795814 0.81      ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.76830704 0.73562645 0.70058101 0.9       ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.80151895 0.85288594 0.87042474 1.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}